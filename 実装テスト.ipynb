{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlxdRcxiNXCBLocjS6Hpaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waseda-ken/data_analyze_kikagaku/blob/main/%E5%AE%9F%E8%A3%85%E3%83%86%E3%82%B9%E3%83%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neGSqECdIH91",
        "outputId": "67f0c8de-a039-46a4-af30-ae8834f2b571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "選択モデル: RandomForest (検証 F1 = 0.3799)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1. データの読み込みとラベル数値化\n",
        "data = pd.read_csv('bank.csv')\n",
        "data['result'] = data['result'].map({'fail': 0, 'success': 1})\n",
        "x = data.drop(columns=['result'])\n",
        "y = data['result']\n",
        "\n",
        "# 2. データ分割 (train:val:test = 60:20:20), 乱数シード0, 層化サンプリング\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=0, stratify=y\n",
        ")\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train_val, y_train_val, test_size=0.25, random_state=0, stratify=y_train_val\n",
        ")\n",
        "\n",
        "# 3. 前処理パイプラインの定義\n",
        "numeric_features = ['age', 'balance', 'pdays', 'previous']\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'poutcome']\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # 数値の欠損を中央値で補完\n",
        "    ('scaler', StandardScaler())                     # 標準化\n",
        "])\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # カテゴリの欠損を \"missing\" で補完\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))                     # ワンホットエンコーディング\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, numeric_features),\n",
        "    ('cat', categorical_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# 4. モデルパイプラインの設定\n",
        "pipeline_lr = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', LogisticRegression(solver='liblinear', random_state=0))\n",
        "])\n",
        "pipeline_rf = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', RandomForestClassifier(n_jobs=-1, random_state=0))\n",
        "])\n",
        "\n",
        "# 5. グリッドサーチ用ハイパーパラメータ\n",
        "param_grid_lr = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10],\n",
        "    'clf__class_weight': [None, 'balanced']\n",
        "}\n",
        "param_grid_rf = {\n",
        "    'clf__n_estimators': [100, 200],\n",
        "    'clf__max_depth': [None, 10, 20],\n",
        "    'clf__class_weight': [None, 'balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "# 6. 5-fold CV で F1 最大化を目指すグリッドサーチ\n",
        "search_lr = GridSearchCV(pipeline_lr, param_grid_lr, scoring='f1', cv=5, n_jobs=-1)\n",
        "search_lr.fit(x_train, y_train)\n",
        "\n",
        "search_rf = GridSearchCV(pipeline_rf, param_grid_rf, scoring='f1', cv=5, n_jobs=-1)\n",
        "search_rf.fit(x_train, y_train)\n",
        "\n",
        "# 7. 検証データで F1 を比較し、ベストモデルを選択\n",
        "f1_lr = f1_score(y_val, search_lr.predict(x_val))\n",
        "f1_rf = f1_score(y_val, search_rf.predict(x_val))\n",
        "\n",
        "if f1_lr >= f1_rf:\n",
        "    best_model = search_lr.best_estimator_\n",
        "    model_name = 'LogisticRegression'\n",
        "    best_f1 = f1_lr\n",
        "else:\n",
        "    best_model = search_rf.best_estimator_\n",
        "    model_name = 'RandomForest'\n",
        "    best_f1 = f1_rf\n",
        "\n",
        "print(f\"選択モデル: {model_name} (検証 F1 = {best_f1:.4f})\")\n",
        "\n",
        "# 8. 訓練＋検証データ全体で再学習\n",
        "best_model.fit(x_train_val, y_train_val)\n",
        "\n",
        "# 9. 推論用データの予測と文字列変換\n",
        "pred_data = pd.read_csv('bank_pred.csv')\n",
        "y_pred_num = best_model.predict(pred_data)\n",
        "y_pred_label = ['success' if y==1 else 'fail' for y in y_pred_num]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. submission.csv に書き出し\n",
        "output_df = pd.DataFrame({'pred': y_pred_label})\n",
        "output_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"推論結果を submission.csv に保存しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND7eXPisIS3c",
        "outputId": "1da2976a-bc83-4805-90c8-a1be162069fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "推論結果を submission.csv に保存しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "from xgboost import xGBClassifier\n",
        "\n",
        "# 1. データ読み込みとラベル数値化\n",
        "data = pd.read_csv('bank.csv')\n",
        "data['result'] = data['result'].map({'fail': 0, 'success': 1})\n",
        "x = data.drop(columns=['result'])\n",
        "y = data['result']\n",
        "\n",
        "# 2. train:val:test = 60:20:20、乱数シード＝0、層化サンプリング\n",
        "x_tv, x_test, y_tv, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=0, stratify=y\n",
        ")\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_tv, y_tv, test_size=0.25, random_state=0, stratify=y_tv\n",
        ")\n",
        "\n",
        "# 3. 前処理パイプライン\n",
        "num_feats = ['age', 'balance', 'pdays', 'previous']\n",
        "cat_feats = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'poutcome']\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "cat_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipe, num_feats),\n",
        "    ('cat', cat_pipe, cat_feats)\n",
        "])\n",
        "\n",
        "# 4. xGBoost パイプライン＆ハイパーパラグリッド\n",
        "pipe_xgb = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', xGBClassifier(\n",
        "        eval_metric='logloss',\n",
        "        random_state=0,\n",
        "        use_label_encoder=False  # 古いバージョン互換で残せますが警告が出るので削除してもOK\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__n_estimators':    [100, 200],\n",
        "    'clf__max_depth':       [3, 5],\n",
        "    'clf__learning_rate':   [0.01, 0.1],\n",
        "    'clf__scale_pos_weight': [1, (y_train==0).sum()/(y_train==1).sum()]\n",
        "}\n",
        "\n",
        "search = GridSearchCV(\n",
        "    pipe_xgb, param_grid,\n",
        "    scoring='f1', cv=5, n_jobs=-1, verbose=1\n",
        ")\n",
        "search.fit(x_train, y_train)\n",
        "\n",
        "# 5. 検証データで最適閾値を探索\n",
        "probs_val = search.predict_proba(x_val)[:, 1]\n",
        "best_thr, best_f1 = 0.5, f1_score(y_val, (probs_val > 0.5).astype(int))\n",
        "for thr in np.linspace(0.1, 0.9, 81):\n",
        "    f1 = f1_score(y_val, (probs_val > thr).astype(int))\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_thr = f1, thr\n",
        "\n",
        "print(f\"Best params: {search.best_params_}\")\n",
        "print(f\"Validation F1 @ threshold {best_thr:.2f} = {best_f1:.4f}\")\n",
        "\n",
        "# 6. train+val 全体で再学習\n",
        "best_model = search.best_estimator_\n",
        "best_model.fit(x_tv, y_tv)\n",
        "\n",
        "# 7. 提出用データに予測を適用\n",
        "df_pred = pd.read_csv('bank_pred.csv')\n",
        "probs_test = best_model.predict_proba(df_pred)[:, 1]\n",
        "y_test_pred = (probs_test > best_thr).astype(int)\n",
        "labels = ['success' if y==1 else 'fail' for y in y_test_pred]\n",
        "\n",
        "# 8. submission.csv に出力\n",
        "pd.Series(labels, name='pred').to_csv('submission.csv', index=False)\n",
        "print(\"submission.csv を出力しました。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LamSzk7lOrNx",
        "outputId": "fc0f6eae-0f94-4f8d-c608-df17f012c1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:47:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__n_estimators': 100, 'clf__scale_pos_weight': np.float64(7.530805687203792)}\n",
            "Validation F1 @ threshold 0.60 = 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:47:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv を出力しました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-c1tZbpQ-UA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}